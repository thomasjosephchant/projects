{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20234ad1-c45d-497f-81de-2c92b1880ad2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import io\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import spacy\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def s3_event_info(event):\n",
    "    '''\n",
    "    Extracts the S3 bucket name and file key from a json created by an SNS trigger in response to a file being uploaded.\n",
    "    \n",
    "    Parameters: \n",
    "        event (json): a json file created from an SNS trigger. This event is created when a file is uploaded to the bucket to which this lambda will be applied. \n",
    "    \n",
    "    Returns: \n",
    "        bucket_name (str): The s3 bucket name to which the file is uploaded\n",
    "        file_key_attribute (str): The file key directing to the location of the file within the s3 bucket.\n",
    "    '''\n",
    "    \n",
    "    # Get the details of the uploaded file from the event\n",
    "    messagejson = json.loads(event['Records'][0]['Sns']['Message'])\n",
    "    logging.info(\"Event has been loaded in.\")\n",
    "    \n",
    "    # Extract the bucket key\n",
    "    bucket_name = messagejson['Records'][0]['s3']['bucket']['name']\n",
    "    if not isinstance(bucket_name, str):\n",
    "        raise TypeError(\"The bucket_name grabbed was not a string\")\n",
    "    logging.info(f\"The bucket name is: {bucket_name}\")\n",
    "    \n",
    "    file_key_attribute = messagejson['Records'][0]['s3']['object']['key']\n",
    "    if not isinstance(file_key_attribute, str):\n",
    "        raise TypeError(\"The file_key grabbed was not a string\")\n",
    "    logging.info(f\"The file key is: {file_key_attribute}\")\n",
    "    \n",
    "    return [bucket_name, file_key_attribute]\n",
    "\n",
    "def file_downloader(bucket_name, file_key_attribute):\n",
    "    \"\"\"\n",
    "    Download a file from an S3 bucket.\n",
    "\n",
    "    Parameters:\n",
    "    bucket_name (str): The name of the S3 bucket.\n",
    "    file_key_attribute (str): The key of the file in the S3 bucket.\n",
    "\n",
    "    Returns:\n",
    "    body (bytes): The content of the file.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=file_key_attribute)\n",
    "    body = obj['Body'].read()\n",
    "    return body\n",
    "\n",
    "def sheet_name_grabber(iofile):\n",
    "    \"\"\"\n",
    "    Extract sheet names from an Excel file.\n",
    "\n",
    "    Parameters:\n",
    "    iofile (bytes): The content of the Excel file.\n",
    "\n",
    "    Returns:\n",
    "    attribute_ex (openpyxl.workbook.workbook.Workbook): The Workbook object.\n",
    "    sheet_names_attribute (list): A list of sheet names.\n",
    "    \"\"\"\n",
    "    attribute_ex = openpyxl.load_workbook(io.BytesIO(iofile), data_only=True)\n",
    "    sheet_names_attribute = attribute_ex.sheetnames\n",
    "    return attribute_ex, sheet_names_attribute\n",
    "\n",
    "def df_maker(workbook, sheetnamelist):\n",
    "    \"\"\"\n",
    "    Convert each sheet in a Workbook into a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    workbook (openpyxl.workbook.workbook.Workbook): The Workbook object.\n",
    "    sheetnamelist (list): A list of sheet names.\n",
    "\n",
    "    Returns:\n",
    "    dflist (list): A list of DataFrames, each DataFrame corresponding to a sheet in the Workbook.\n",
    "    \"\"\"\n",
    "    dflist = []\n",
    "    for sheet_name in sheetnamelist:\n",
    "        if 'Table' in sheet_name:\n",
    "            ws = workbook[sheet_name]\n",
    "            sheet_name = pd.DataFrame(ws.values)\n",
    "            dflist.append(sheet_name)\n",
    "    return dflist\n",
    "\n",
    "def title_extracter(workbook, sheetnamelist):\n",
    "    \"\"\"\n",
    "    Extract the title from each sheet in a Workbook.\n",
    "\n",
    "    Parameters:\n",
    "    workbook (openpyxl.workbook.workbook.Workbook): The Workbook object.\n",
    "    sheetnamelist (list): A list of sheet names.\n",
    "\n",
    "    Returns:\n",
    "    titles (list): A list of titles.\n",
    "    \"\"\"\n",
    "    titles = []\n",
    "    for sheet_name in sheetnamelist:\n",
    "        if 'Table' in sheet_name:\n",
    "            ws = workbook[sheet_name]\n",
    "            df = pd.DataFrame(ws.values)\n",
    "            titles.append(df.iloc[0, 2])\n",
    "    return titles\n",
    "\n",
    "def df_header_fixer(df):\n",
    "    \"\"\"\n",
    "    Fix the header of a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame to fix.\n",
    "\n",
    "    Returns:\n",
    "    df (pandas.DataFrame): The DataFrame with fixed header.\n",
    "    \"\"\"\n",
    "    df = df.rename(columns=df.iloc[2]).loc[3:]\n",
    "    return df\n",
    "\n",
    "def df_trunkater(df):\n",
    "    \"\"\"\n",
    "    Remove empty rows in a DataFrame and reset its index.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame to truncate.\n",
    "\n",
    "    Returns:\n",
    "    df (pandas.DataFrame): The DataFrame without empty rows and with reset index.\n",
    "    \"\"\"\n",
    "    df = df[df['Attribute_Name'].notna()]\n",
    "    df.index = np.arange(len(df))\n",
    "    return df\n",
    "\n",
    "def desc_filler(df):\n",
    "    \"\"\"\n",
    "    Fill empty description cells in a DataFrame with corresponding attribute names.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame to fill.\n",
    "\n",
    "    Returns:\n",
    "    df (pandas.DataFrame): The DataFrame with filled description cells.\n",
    "    \"\"\"\n",
    "    df = df[df['Attribute_Name'].notna()]\n",
    "    df['Description'].fillna(df['Attribute_Name'], inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed1cc265-164b-47f5-a352-c234926fb85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The following functions are used for the clusterer'''\n",
    "\n",
    "\n",
    "def km_clusterer(X, cluster_count):\n",
    "    '''\n",
    "    Peform K-means clustering on the input data and finds the optimal number of clusters using silhouette scores.\n",
    "    \n",
    "    Parameters:\n",
    "        X (vector): The input vector\n",
    "        cluster_count (int): The initialized value for the optimal K.\n",
    "        \n",
    "    Returns:\n",
    "        best_k (int): The optimal K value to maximize the silhouette score for K-means on the vector.\n",
    "    '''\n",
    "    # Define a range of k values to try\n",
    "    k_values = range(2, 11)\n",
    "\n",
    "    # Initialize lists to store silhouette scores and sample silhouette values\n",
    "    silhouette_scores = []\n",
    "    sample_silhouette_values = []\n",
    "\n",
    "    # Iterate over different values of k\n",
    "    for k in k_values:\n",
    "        # Fit k-means clustering model\n",
    "        kmeans = KMeans(n_clusters=k, n_init = 'auto', random_state=42)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "\n",
    "        # Compute the silhouette score for this clustering\n",
    "        silhouette_avg = silhouette_score(X, labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "        # Compute the silhouette values for each sample\n",
    "        sample_silhouette_values.append(silhouette_samples(X, labels))\n",
    "\n",
    "    # Find the best k based on the maximum silhouette score\n",
    "    best_k = k_values[np.argmax(silhouette_scores)]\n",
    "\n",
    "    return best_k\n",
    "\n",
    "\n",
    "def kmeansfunc(df, vec, cluster_count):\n",
    "    '''\n",
    "    Apply K-means clustering to the input dataframe and assign cluster label.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): The input dataframe.\n",
    "        vec (array-like): The input vector.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: The dataframe with cluster labels added.\n",
    "    '''\n",
    "    k = cluster_count\n",
    "    kmeans = KMeans(n_clusters=k, n_init='auto', random_state=42)\n",
    "    y_pred = kmeans.fit_predict(vec)\n",
    "    df['y'] = y_pred\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def tsne_transformer(vector):\n",
    "    '''\n",
    "    Perform t-SNE transformation on a vector\n",
    "    \n",
    "    Parameters:\n",
    "        vector (array-like): The input vector.\n",
    "    \n",
    "    Returns:\n",
    "        array-like: The transformed vector.\n",
    "    '''\n",
    "    tsne = TSNE(perplexity=20) #verbose=1, \n",
    "    vector = tsne.fit_transform(vector)\n",
    "\n",
    "    return vector\n",
    "\n",
    "def kmeans_plotter(df, vec):\n",
    "    '''\n",
    "    Create a scatterplot of the t-SNE'd data with labels\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): The input dataframe.\n",
    "        vec (array-like): The input vector.\n",
    "    '''\n",
    "    X_embedded = vec\n",
    "    y_pred = df['y']\n",
    "    m = max(df['y'])+1\n",
    "    # sns settings\n",
    "    sns.set(rc={'figure.figsize': (13, 9)})\n",
    "\n",
    "    # colors\n",
    "    palette = sns.hls_palette(m, l=.4, s=.9)\n",
    "\n",
    "    # plot\n",
    "    sns.scatterplot(x=X_embedded[:, 0], y=X_embedded[:, 1], hue=y_pred, legend='full', palette=palette,)  \n",
    "    plt.title('t-SNE with Kmeans Labels')\n",
    "    plt.savefig(\"improved_cluster_tsne.png\")\n",
    "    plt.show()\n",
    "    \n",
    "def vectorizer_lister(df, cluster_count):\n",
    "    '''\n",
    "    Create a list of vectorizers for a dataframe, one for each cluster\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): The input dataframe.\n",
    "        cluster_count (int): The number of clusters.\n",
    "    \n",
    "    Returns:\n",
    "        list: The list of vectorizers.\n",
    "    '''\n",
    "    vectorizers = []\n",
    "    for ii in range(0, cluster_count):\n",
    "        vectorizers.append(TfidfVectorizer(lowercase=True, stop_words='english', ngram_range = (1, 4)))\n",
    "\n",
    "    return vectorizers\n",
    "\n",
    "def data_vectorizer(df, vectorizers):\n",
    "\n",
    "    '''\n",
    "    Vectorize the data in each dataframe\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): The input dataframe.\n",
    "        vectorizers (list): The list of vectorizers.\n",
    "    \n",
    "    Returns:\n",
    "        list: The list of vectorized data.\n",
    "    '''\n",
    "    vectorized_data = []\n",
    "    for current_cluster, cvec in enumerate(vectorizers):\n",
    "        vectorized_data.append(cvec.fit_transform(df.loc[df['y'] == current_cluster, 'Cleaned_Text']))\n",
    "\n",
    "    return vectorized_data\n",
    "\n",
    "def ld_allocater(df, cluster_count):\n",
    "    '''\n",
    "    Create a list of latent Dirichlet allocation models, one for each cluster\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): The input dataframe.\n",
    "        cluster_count (int): The number of clusters.\n",
    "    \n",
    "    Returns:\n",
    "        list: The list of latent Dirichlet allocation models.\n",
    "    '''\n",
    "\n",
    "    NUM_TOPICS_PER_CLUSTER = 1\n",
    "    lda_models = []\n",
    "        \n",
    "    for ii in range(0, cluster_count):\n",
    "        lda = LatentDirichletAllocation(n_components=NUM_TOPICS_PER_CLUSTER, max_iter=10, learning_method='online', verbose=False, random_state=42)\n",
    "        lda_models.append(lda)\n",
    "    return lda_models\n",
    "\n",
    "def lda_clusterer(lda_models, vectorized_data):\n",
    "    '''\n",
    "    Apply LDA clustering to vectorized data.\n",
    "\n",
    "    Parameters:\n",
    "        lda_models (list): The list of Latent Dirichlet Allocation models.\n",
    "        vectorized_data (list): The list of vectorized data.\n",
    "\n",
    "    Returns:\n",
    "        list: The list of LDA-clustered data.\n",
    "    '''\n",
    "    clusters_lda_data = []\n",
    "\n",
    "    for current_cluster, lda in enumerate(lda_models):\n",
    "        if vectorized_data[current_cluster] is not None:\n",
    "            clusters_lda_data.append(lda.fit_transform(vectorized_data[current_cluster]))\n",
    "    return clusters_lda_data\n",
    "\n",
    "def lda_scorer(lda_models, vectorized_data):\n",
    "    '''\n",
    "    Calculates the scores for LDA models based on the vectorized data.\n",
    "\n",
    "    Parameters:\n",
    "        lda_models (list): List of LDA models.\n",
    "        vectorized_data (list): List of vectorized data.\n",
    "\n",
    "    Returns:\n",
    "        list: List of scores for each LDA model.\n",
    "\n",
    "    '''\n",
    "    scores_lda_data = []\n",
    "\n",
    "    for current_score, lda in enumerate(lda_models):\n",
    "        if vectorized_data[current_score] is not None:\n",
    "            scores_lda_data.append(lda.score(vectorized_data[current_score]))\n",
    "    \n",
    "    return scores_lda_data\n",
    "\n",
    "\n",
    "\n",
    "def selected_topics(model, vectorizer, top_n=50):\n",
    "    '''\n",
    "    Function for printing keywords for each topic.\n",
    "\n",
    "    Parameters:        \n",
    "        model: The LDA model.\n",
    "        vectorizer: The vectorizer.\n",
    "        top_n (int): The number of top keywords to return for each topic.\n",
    "\n",
    "    Returns:\n",
    "        list: The list of top keywords for each topic.\n",
    "    '''\n",
    "    current_words = []\n",
    "    keywords = []\n",
    "    scores = []\n",
    "\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        words = [(vectorizer.get_feature_names_out()[i], topic[i]) for i in topic.argsort()[:-top_n - 1:-1]] #returns the words corresponding to the top_n scores for each topic\n",
    "            \n",
    "        for word in words:\n",
    "            if word[0] not in current_words:  #Unpacks the vector to extract the most relevant tags\n",
    "                keywords.append(word)\n",
    "                current_words.append(word[0]) #avoid adding repeat terms by keeping a second list with only the words\n",
    "\n",
    "    keywords.sort(key=lambda x: x[1])\n",
    "    keywords.reverse()\n",
    "    return_values = []\n",
    "    for ii in keywords:\n",
    "        return_values.append(ii)\n",
    "\n",
    "    return return_values\n",
    "\n",
    "\n",
    "def keyword_lister(lda_models, vectorized_data, vectorizers):\n",
    "    '''\n",
    "    Compile keywords for each table.\n",
    "\n",
    "    Parameters:\n",
    "        lda_models (list): The list of LDA models.\n",
    "        vectorized_data (list): The list of vectorized data.\n",
    "        vectorizers (list): The list of vectorizers.\n",
    "\n",
    "    Returns:\n",
    "        list: The list of keywords for each table.\n",
    "    '''\n",
    "    all_keywords = []\n",
    "    for current_vectorizer, lda in enumerate(lda_models):\n",
    "        if vectorized_data[current_vectorizer] is not None:\n",
    "            all_keywords.append(selected_topics(lda, vectorizers[current_vectorizer]))\n",
    "    termslist = []\n",
    "    scoreslist = []\n",
    "    \n",
    "    return all_keywords\n",
    "\n",
    "def unpack_tags(df):\n",
    "    '''\n",
    "    Unpacks the tags in a dataframe by creating a new dataframe with each tag as a separate row.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The input dataframe containing 'Table_Number' and 'Tags' columns.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The unpacked dataframe with 'Table_Number' and 'Tags' columns.\n",
    "\n",
    "    '''\n",
    "    # Create empty lists to store the unpacked tags and table numbers\n",
    "    unpacked_tags = []\n",
    "    table_numbers = []\n",
    "\n",
    "    # Iterate through each row in the dataframe\n",
    "    for _, row in df.iterrows():\n",
    "        tags = row['Tags']\n",
    "        table_number = row['Table_Number']\n",
    "\n",
    "        # Iterate through each sublist of tags\n",
    "        for sublist in tags:\n",
    "            # Extend the unpacked_tags list with each tag from the sublist\n",
    "            unpacked_tags.extend(sublist)\n",
    "\n",
    "            # Extend the table_numbers list with the corresponding table number\n",
    "            table_numbers.extend([table_number] * len(sublist))\n",
    "    \n",
    "    # Create a new dataframe with columns 'Table_Number' and 'Tags' containing the unpacked tags\n",
    "    df_unpacked = pd.DataFrame({'Table_Number': table_numbers, 'Tags': unpacked_tags})\n",
    "    return df_unpacked\n",
    "\n",
    "\n",
    "def minmaxscaler(df):\n",
    "    '''\n",
    "    Scales the scores in the dataframe using min-max scaling.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The input dataframe with a 'Score' column.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The dataframe with the scaled 'Score' column.\n",
    "\n",
    "    '''\n",
    "    if 'Scores' in df.columns:\n",
    "        df['Score'] = df['Scores']\n",
    "        df = df.drop('Scores', axis=1)\n",
    "    \n",
    "    df['Score'] = df['Score'] / max(df['Score'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def dfcombiner(df1, df2):\n",
    "    '''\n",
    "    Combines two dataframes based on the 'Tags' column and calculates a final score.\n",
    "\n",
    "    Parameters:\n",
    "        df1 (DataFrame): The first input dataframe.\n",
    "        df2 (DataFrame): The second input dataframe.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The combined dataframe with a calculated 'Final_Score' column.\n",
    "\n",
    "    '''\n",
    "    df = pd.merge(df1, df2, on='Tags', how='outer')\n",
    "\n",
    "    min_score_y = df['Score_y'].min()\n",
    "    min_score_x = df['Score_x'].min()\n",
    "\n",
    "    df['Score_y'].fillna(min_score_y, inplace=True)\n",
    "    df['Score_x'].fillna(min_score_x, inplace=True)\n",
    "\n",
    "    df['Final_Score'] = (df['Score_y'] + df['Score_x']) / 2\n",
    "\n",
    "    df.drop('Score_x', axis=1, inplace=True)\n",
    "    df.drop('Score_y', axis=1, inplace=True)\n",
    "\n",
    "    df['Table_Number'] = radcom_df['Table_Number_y'].combine_first(df['Table_Number_x'])\n",
    "    df.drop('Table_Number_x', axis=1, inplace=True)\n",
    "    df.drop('Table_Number_y', axis=1, inplace=True)\n",
    "\n",
    "    df = df.sort_values(by='Final_Score', ascending=False)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60a96e6c-c35c-4c18-87ef-7a894b148a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "\n",
    "'''These functions are used to create word vectors for the data, including tf-idf vectorization'''\n",
    "\n",
    "def reference_grabber():\n",
    "    \"\"\"Retrieve and process data from CSV files based on file keys.\n",
    "\n",
    "    Reads file keys from 'file_keys_refs.json' to download and process CSV files\n",
    "    from a specified bucket. The CSV files contain abbreviation and definition data.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing two pandas DataFrames.\n",
    "            - The first DataFrame (abbdf) contains the abbreviation data with columns 'Abbreviation' and 'Term'.\n",
    "            - The second DataFrame (defsdf) contains the definition data with columns 'Term' and 'Definition'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load file keys from 'file_keys_refs.json'\n",
    "    file_name = 'file_keys_refs.json'\n",
    "    with open(file_name, 'r') as f:\n",
    "        file_keys = json.load(f)\n",
    "\n",
    "    # Extract bucket name and file keys for abbreviation and definition data\n",
    "    bucket_name = file_keys['bucket_name']\n",
    "    abb_key = file_keys['file_key_abb']\n",
    "    defs_key = file_keys['file_key_defs']\n",
    "\n",
    "    # Download and process abbreviation data\n",
    "    abbdf = file_downloader(bucket_name, abb_key)\n",
    "    abbdf = pd.read_csv(io.BytesIO(abbdf))\n",
    "    abbdf = abbdf.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "    # Download and process definition data\n",
    "    defsdf = file_downloader(bucket_name, defs_key)\n",
    "    defsdf = pd.read_csv(io.BytesIO(defsdf))\n",
    "    defsdf = defsdf.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "    # Convert abbreviation and term data to lowercase\n",
    "    abbdf['Abbreviation'] = abbdf['Abbreviation'].str.lower()\n",
    "    defsdf['Term'] = defsdf['Term'].str.lower()\n",
    "\n",
    "    return abbdf, defsdf\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()\n",
    "\n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    '''\n",
    "    Remove spaces and convert text into lowercase\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): The input text to be cleaned.\n",
    "    \n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    '''\n",
    "    return text.strip().lower()\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    '''\n",
    "    Tokenize a sentence using spaCy, performing lemmatization, lowercase conversion, and removing stop words.\n",
    "    \n",
    "    Parameters:\n",
    "        sentence (str): The input sentence to be tokenized.\n",
    "    \n",
    "    Returns:\n",
    "        list: The list of tokens.\n",
    "    '''\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "    \n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    for word in mytokens:\n",
    "        if word.lemma_ != \"-PRON-\":\n",
    "            word.lemma_.lower().strip()\n",
    "        else:\n",
    "            word = word.lower_\n",
    "    \n",
    "    # Removing stop words and punctuation\n",
    "    mytokens = [word for word in mytokens if word not in STOP_WORDS and not word.is_punct]\n",
    "\n",
    "    # Return preprocessed list of tokens\n",
    "    return mytokens\n",
    "\n",
    "# Custom transformer using spaCy\n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        '''\n",
    "        Transform the input X by cleaning the text.\n",
    "        \n",
    "        Parameters:\n",
    "            X (list): The input list of texts to be transformed.\n",
    "        \n",
    "        Returns:\n",
    "            list: The transformed list of cleaned texts.\n",
    "        '''\n",
    "        # Cleaning Text\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        '''\n",
    "        Fit the transformer.\n",
    "        \n",
    "        Parameters:\n",
    "            X (list): The input list of texts.\n",
    "            y: Ignored parameter.\n",
    "        \n",
    "        Returns:\n",
    "            self: The fitted transformer object.\n",
    "        '''\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        '''\n",
    "        Get parameters for the transformer.\n",
    "        \n",
    "        Parameters:\n",
    "            deep: Ignored parameter.\n",
    "        \n",
    "        Returns:\n",
    "            dict: The parameters of the transformer.\n",
    "        '''\n",
    "        return {}\n",
    "\n",
    "def cleaner(df):\n",
    "    '''\n",
    "    Clean the 'Description' column in the dataframe by applying the clean_text function.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): The input dataframe.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: The dataframe with the 'Cleaned_Text' column added.\n",
    "    '''\n",
    "    df['Cleaned_Text'] = df['Description'].apply(clean_text)\n",
    "    return df\n",
    "\n",
    "tfidf_vector = sklearn.feature_extraction.text.TfidfVectorizer(max_df = 0.9, ngram_range = (1,4), stop_words = 'english')\n",
    "\n",
    "def vectorizer(df):\n",
    "    '''\n",
    "    Vectorize the 'Cleaned_Text' column of the dataframe using TF-IDF vectorization.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): The input dataframe.\n",
    "    \n",
    "    Returns:\n",
    "        array-like: The vectorized data.\n",
    "    '''\n",
    "    X = df['Cleaned_Text']\n",
    "    X = tfidf_vector.fit_transform(X)\n",
    "    feature_names = tfidf_vector.get_feature_names_out()\n",
    "    return X, feature_names\n",
    "\n",
    "def chunkvectorizer(df):\n",
    "    '''\n",
    "    Vectorize the 'Chunks' column of the dataframe using TF-IDF vectorization.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): The input dataframe.\n",
    "    \n",
    "    Returns:\n",
    "        array-like: The vectorized data.\n",
    "    '''\n",
    "    doctext = df['Cleaned_Text'].apply(nlp)\n",
    "    noun_phrases = []\n",
    "    for i in doctext:\n",
    "        current_list = []\n",
    "        for chunk in i.noun_chunks:\n",
    "            current_list.append(chunk)\n",
    "        noun_phrases.append(current_list)\n",
    "    df['Chunks'] = noun_phrases\n",
    "    tsl = []\n",
    "    noun_phrases = df['Chunks']\n",
    "    for i in noun_phrases:\n",
    "        token_strings = [token.text for token in i]\n",
    "        tsl.append(token_strings)\n",
    "    chunklist = []\n",
    "    \n",
    "    for lisst in tsl:\n",
    "        for chunk in lisst:\n",
    "            chunklist.append(chunk)\n",
    "    print(chunklist)\n",
    "    vectorizertest = TfidfVectorizer()\n",
    "    X = vectorizertest.fit_transform(chunklist)    \n",
    "    return X\n",
    "\n",
    "def pcareducer(X):\n",
    "    '''\n",
    "    Reduce the dimensionality of the input data using PCA.\n",
    "    \n",
    "    Parameters:\n",
    "        X (array-like): The input data to be reduced.\n",
    "    \n",
    "    Returns:\n",
    "        array-like: The reduced data.\n",
    "    '''\n",
    "    pca = PCA(n_components=0.95, random_state=42)\n",
    "    X_reduced = pca.fit_transform(X.toarray())\n",
    "    return X_reduced\n",
    "\n",
    "def vectortodf(X, feature_names):\n",
    "    '''\n",
    "    Find the words with TF-IDF values >= threshold from the vectorized data.\n",
    "    \n",
    "    Parameters:\n",
    "        X (array-like): The vectorized data.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: The dataframe containing the words and their TF-IDF scores.\n",
    "    '''\n",
    "    goodwords = []\n",
    "    goodvals = []\n",
    "    \n",
    "    for row in range(X.shape[0]):  # Iterate over each row in the matrix\n",
    "        for col in X[row, :].nonzero()[1]:  # Get the indices of non-zero elements in the row\n",
    "            word = feature_names[col]\n",
    "            \n",
    "            if word not in goodwords:\n",
    "                goodwords.append(word)\n",
    "                goodvals.append(X[row, col])\n",
    "                  \n",
    "    tagsdf = pd.DataFrame(data={'Tags': goodwords, 'Score': goodvals})\n",
    "  \n",
    "    return tagsdf\n",
    "\n",
    "def cosvectortodf(X, feature_names):\n",
    "    '''\n",
    "    Converts a cosine similarity matrix and feature names to a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        X (numpy.ndarray): The cosine similarity matrix.\n",
    "        feature_names (list): List of feature names.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with 'Tags' and 'Score' columns.\n",
    "\n",
    "    '''\n",
    "    X = X.transpose()\n",
    "    df = pd.DataFrame({'Tags': feature_names})\n",
    "\n",
    "    similarity_scores = cosine_similarity(X)\n",
    "\n",
    "    scores = similarity_scores.sum(axis=1)\n",
    "\n",
    "    df = pd.DataFrame({'Tags': feature_names, 'Score': scores})\n",
    "\n",
    "    df = df.sort_values(by='Score', ascending=False)\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def dflistconcat(dflist):\n",
    "    '''\n",
    "    Concatenates a list of DataFrames into a single DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        dflist (list): List of DataFrames.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The concatenated DataFrame.\n",
    "\n",
    "    '''\n",
    "    Tablenum = 1\n",
    "    for i in dflist:\n",
    "        i['Table_Number'] = Tablenum\n",
    "        Tablenum += 1\n",
    "    df = pd.concat(dflist)\n",
    "    cols = list(df.columns)\n",
    "    cols.reverse()\n",
    "    df = df[cols]\n",
    "    return df\n",
    "\n",
    "def eliminate_contained_words(df):\n",
    "    '''\n",
    "    Eliminates rows containing tags that are subsets of other tags within each group.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The input DataFrame with 'Tags' column.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The filtered DataFrame without contained words.\n",
    "\n",
    "    '''\n",
    "    def eliminate_within_group(group):\n",
    "        tags = group['Tags'].tolist()\n",
    "        indices_to_remove = []\n",
    "\n",
    "        for i, tag in enumerate(tags):\n",
    "            if any(tag in other_tag for j, other_tag in enumerate(tags) if j != i):\n",
    "                indices_to_remove.append(i)\n",
    "\n",
    "        return group.drop(group.index[indices_to_remove])\n",
    "\n",
    "    if 'Table_Number' in df.columns:\n",
    "        df_filtered = df.groupby('Table_Number').apply(eliminate_within_group).reset_index(drop=True)\n",
    "    if 'Table_Name' in df.columns:\n",
    "        df_filtered = df.groupby('Table_Name').apply(eliminate_within_group).reset_index(drop=True)\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "def termchecker(df, abbdic, termdic):\n",
    "    '''\n",
    "    Adjusts the scores in the DataFrame based on matching tags with abbreviation and term dictionaries.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The input DataFrame with 'Tags' and 'Score' columns.\n",
    "        abbdic (DataFrame): The abbreviation dictionary with 'Abbreviation' column.\n",
    "        termdic (DataFrame): The term dictionary with 'Term' column.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with adjusted scores.\n",
    "\n",
    "    '''\n",
    "    multiplier = max(df['Score'] / df['Score'].mean())\n",
    "    df['Score'] = df.apply(lambda row: row['Score'] * multiplier if row['Tags'] in abbdic['Abbreviation'].values else row['Score'], axis=1)\n",
    "    df['Score'] = df.apply(lambda row: row['Score'] * multiplier if row['Tags'] in termdic['Term'].values else row['Score'], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def topscorefilter(df, numberoftags):\n",
    "    '''\n",
    "    Filters the DataFrame to retain the top tags based on the specified number of tags.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The input DataFrame with 'Score' column.\n",
    "        numberoftags (int): The number of top tags to retain.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The filtered DataFrame with the top tags.\n",
    "\n",
    "    '''\n",
    "    sort_column = 'Ensemble_Score' if 'Ensemble_Score' in df.columns else 'Score'\n",
    "    group_column = 'Table_Name' if 'Table_Name' in df.columns else 'Table_Number'\n",
    "\n",
    "    def filter_within_group(group):\n",
    "        return group.nlargest(numberoftags, sort_column)\n",
    "\n",
    "    df_filtered = df.groupby(group_column).apply(filter_within_group)\n",
    "\n",
    "    df_filtered = df_filtered.reset_index(drop=True)\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "def replace_table_number_with_title(df, titles_list):\n",
    "    '''\n",
    "    Replace the values in the \"Table_Number\" column of the DataFrame with the corresponding titles from the titles list.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The DataFrame containing the \"Table_Number\" column.\n",
    "        titles_list (list): The list of titles corresponding to the table numbers.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The modified DataFrame with the \"Table_Number\" column replaced by titles.\n",
    "    '''\n",
    "    df['Table_Name'] = df['Table_Number'].map(lambda x: titles_list[int(x) - 1])\n",
    "    df.drop('Table_Number', axis = 1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def quanttrans(df):\n",
    "    '''\n",
    "    Apply quantile transformation to the 'Score' column of the input DataFrame.\n",
    "\n",
    "    This function uses QuantileTransformer from scikit-learn to perform quantile transformation,\n",
    "    which maps the data to a normal distribution. The number of quantiles used is determined by\n",
    "    the length of the DataFrame divided by 4.\n",
    "\n",
    "    Note: This function modifies the original DataFrame by replacing the 'Score' column.\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): The input DataFrame containing the 'Score' column to be transformed.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame with the 'Score' column quantile-transformed.\n",
    "    '''\n",
    "    qt = QuantileTransformer(n_quantiles = int(len(df)/4), random_state=42, output_distribution='normal')\n",
    "    X = df['Score'].values.reshape(-1, 1)  # Reshape to a 2D array\n",
    "    X = qt.fit_transform(X)\n",
    "    df['Score'] = X\n",
    "    return df\n",
    "\n",
    "def get_characters_before_slash(input_string):\n",
    "    \"\"\"\n",
    "    Get all the characters in a string before the first '/' character.\n",
    "    This is used to find the folder name that a file was downloaded from so that it can be catalogged in the database\n",
    "    \n",
    "    Parameters:\n",
    "        input_string (str): The input string from which characters will be extracted.\n",
    "\n",
    "    Returns:\n",
    "        str: The substring of the input string from the beginning up to, but not including,\n",
    "             the first occurrence of the '/' character. If the '/' character is not present,\n",
    "             the entire input string is returned.\n",
    "    \"\"\"\n",
    "    index = input_string.find('/')\n",
    "    if index != -1:\n",
    "        return input_string[:index]\n",
    "    else:\n",
    "        return input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0a58cb6-90c0-412d-bbb3-a306847a0a4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def import_pipeline(bucket, file_key):\n",
    "    '''\n",
    "    Pipeline the import process, beginning with an S3 file location and eventually outputting a list of dataframes, one for each table in the spreadsheet.\n",
    "    \n",
    "    Parameters:\n",
    "        bucket (str): The S3 bucket name.\n",
    "        file_key (str): The file key directing to the location of the file within the S3 bucket.\n",
    "    \n",
    "    Returns:\n",
    "        list: The list of dataframes, one for each table in the spreadsheet.\n",
    "    '''\n",
    "    body = file_downloader(bucket, file_key)\n",
    "    workbook, sheet_names = sheet_name_grabber(body)\n",
    "    dflist = df_maker(workbook, sheet_names)\n",
    "    titles = title_extracter(workbook, sheet_names)\n",
    "    return dflist, titles\n",
    "\n",
    "def import_pipeline_local(local_file_location):\n",
    "    '''\n",
    "    Pipeline the import process, beginning with an S3 file location and eventually outputting a list of dataframes, one for each table in the spreadsheet.\n",
    "    \n",
    "    Parameters:\n",
    "        bucket (str): The S3 bucket name.\n",
    "        file_key (str): The file key directing to the location of the file within the S3 bucket.\n",
    "    \n",
    "    Returns:\n",
    "        list: The list of dataframes, one for each table in the spreadsheet.\n",
    "    '''\n",
    "    body = local_file_location\n",
    "    workbook, sheet_names = sheet_name_grabber(body)\n",
    "    dflist = df_maker(workbook, sheet_names)\n",
    "    titles = title_extracter(workbook, sheet_names)\n",
    "    return dflist, titles\n",
    "\n",
    "def pipeline(dataframelist, functions):\n",
    "    '''\n",
    "    Apply a list of functions to each dataframe in the input list.\n",
    "    \n",
    "    Parameters:\n",
    "        dataframelist (list): The list of dataframes.\n",
    "        functions (list): The list of functions to be applied to each dataframe.\n",
    "    \n",
    "    Returns:\n",
    "        list: The list of processed dataframes.\n",
    "    '''\n",
    "    results = []\n",
    "    for df in dataframelist:\n",
    "        for func in functions:\n",
    "            df = func(df)  # Apply each function in sequence\n",
    "        results.append(df)  # Add each processed dataframe to the list of processed dataframes\n",
    "    return results  # Return the list of processed dataframes\n",
    "\n",
    "def cluster_pipeline(dflist, vectorlist, cluster_count):\n",
    "\n",
    "    '''\n",
    "    Run the pipeline to perform clustering and obtain tags for each dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "        dflist (list): The list of dataframes.\n",
    "        vectorlist (list): The list of vectorized data.\n",
    "        cluster_count (int): The initialized number of clusters.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: The dataframe with tags for each table.\n",
    "    '''\n",
    "\n",
    "    #Find the optimal number of clusters for the model, effectively creating our number of topics\n",
    "    clustervectorlist = vectorlist.copy()\n",
    "    \n",
    "    for i in range(len(vectorlist)):\n",
    "        clustervectorlist[i] = pcareducer(clustervectorlist[i])\n",
    "    \n",
    "    cluster_count = []\n",
    "    for i in range(len(vectorlist)):\n",
    "        cluster_count.append(km_clusterer(clustervectorlist[i], cluster_count))\n",
    "\n",
    "    # Perform K-Means clustering on each dataframe\n",
    "    for i in range(len(dflist)):\n",
    "        dflist[i] = kmeansfunc(dflist[i], clustervectorlist[i], cluster_count[i])\n",
    "\n",
    "    # Perform t-SNE transformation on each vector\n",
    "    for i in range(len(clustervectorlist)):\n",
    "        clustervectorlist[i] = tsne_transformer(clustervectorlist[i])\n",
    "  \n",
    "    # Create lists of vectorizers\n",
    "    vectorizers = []\n",
    "    for i in range(len(dflist)):\n",
    "        vectorizers.append(vectorizer_lister(dflist[i], cluster_count[i]))\n",
    "\n",
    "    # Vectorize the data\n",
    "    vectorized_data = []\n",
    "    for i in range(len(dflist)):\n",
    "        vectorized_data.append(data_vectorizer(dflist[i], vectorizers[i]))\n",
    "\n",
    "    # Make LDA Models\n",
    "    lda_models = []\n",
    "    for i in range(len(dflist)):\n",
    "        lda_models.append(ld_allocater(dflist[i], cluster_count[i]))\n",
    "\n",
    "    # Make the LDA Clusters\n",
    "    lda_clusters = []\n",
    "    for i in range(len(lda_models)):\n",
    "        lda_clusters.append(lda_clusterer(lda_models[i], vectorized_data[i]))\n",
    "    \n",
    "    #Test the scorer\n",
    "    lda_scores = []    \n",
    "    for i in range(len(lda_models)):\n",
    "        lda_scores.append(lda_scorer(lda_models[i], vectorized_data[i])) \n",
    "\n",
    "    # Get the keywords\n",
    "    keywords = []\n",
    "    scorelist = []\n",
    "    for i in range(len(lda_models)):\n",
    "        keywords.append(keyword_lister(lda_models[i], vectorized_data[i], vectorizers[i]))\n",
    "    tagslist = keywords\n",
    "\n",
    "    # Make the Dataframes\n",
    "    tablenums = []\n",
    "    for i in range(len(dflist)):\n",
    "\n",
    "        tablenums.append(i + 1)\n",
    "  \n",
    "    df = pd.DataFrame({'Table_Number': tablenums, 'Tags': keywords})\n",
    "    \n",
    "    # eliminate the duplicate keywords\n",
    "    df = unpack_tags(df)\n",
    "    df['Score'] = df['Tags'].apply(lambda x: x[1])\n",
    "    df['Tags'] = df['Tags'].apply(lambda x: x[0])\n",
    "    df = eliminate_contained_words(df)\n",
    "    df = minmaxscaler(df)\n",
    "    df = quanttrans(df)\n",
    " \n",
    "    return df\n",
    "\n",
    "def cos_pipeline(veclist, feature_names_list, abbdic, termdic):\n",
    "    '''\n",
    "    Performs the cosine similarity pipeline to create a DataFrame with adjusted scores.\n",
    "\n",
    "    Parameters:\n",
    "        veclist (list): List of cosine similarity matrices.\n",
    "        feature_names_list (list): List of feature names for each cosine similarity matrix.\n",
    "        abbdic (DataFrame): The abbreviation dictionary with 'Abbreviation' column.\n",
    "        termdic (DataFrame): The term dictionary with 'Term' column.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The processed DataFrame with adjusted scores.\n",
    "\n",
    "    '''\n",
    "    dflist = []\n",
    "    for i in range(len(veclist)):\n",
    "        dflist.append(cosvectortodf(veclist[i], feature_names_list[i]))\n",
    "    df = dflistconcat(dflist) \n",
    "    df = termchecker(df, abbdic, termdic)\n",
    "    df = eliminate_contained_words(df)\n",
    "    df = minmaxscaler(df)\n",
    "    df = quanttrans(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def tfidf_pipeline(veclist, feature_names_list, abbdic, termdic):\n",
    "    '''\n",
    "    Performs the TF-IDF pipeline to create a DataFrame with adjusted scores.\n",
    "\n",
    "    Parameters:\n",
    "        veclist (list): List of TF-IDF matrices.\n",
    "        feature_names_list (list): List of feature names for each TF-IDF matrix.\n",
    "        abbdic (DataFrame): The abbreviation dictionary with 'Abbreviation' column.\n",
    "        termdic (DataFrame): The term dictionary with 'Term' column.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The processed DataFrame with adjusted scores.\n",
    "\n",
    "    '''\n",
    "    dflist = []\n",
    "    for i in range(len(veclist)):\n",
    "        dflist.append(vectortodf(veclist[i], feature_names_list[i]))\n",
    "    df = dflistconcat(dflist)\n",
    "    df = termchecker(df, abbdic, termdic)\n",
    "    df = eliminate_contained_words(df)\n",
    "    df = minmaxscaler(df)\n",
    "    df = quanttrans(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_tagsdf(clustertagsdf, costagsdf, tfidftagsdf, titleslist, numberoftags): #keep this one\n",
    "    \"\"\"\n",
    "    Generate the 'tagsdf' DataFrame by merging the three input DataFrames and applying filtering based on the ensemble score.\n",
    "\n",
    "    Parameters:\n",
    "        clustertagsdf (DataFrame): DataFrame containing cluster-based tags.\n",
    "        costagsdf (DataFrame): DataFrame containing CO-based tags.\n",
    "        tfidftagsdf (DataFrame): DataFrame containing TF-IDF-based tags.\n",
    "        titleslist (list): List of titles corresponding to the table numbers.\n",
    "        numberoftags (int): Number of top tags per table according to the ensemble score.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The resulting 'tagsdf' DataFrame after merging and filtering.\n",
    "    \"\"\"\n",
    "    # Merge the three dataframes based on 'Tags'\n",
    "    merged_df = clustertagsdf.merge(costagsdf, on='Tags').merge(tfidftagsdf, on='Tags')\n",
    "\n",
    "    # Create the ensemble score as the sum of the three scores\n",
    "    merged_df['Ensemble_Score'] = merged_df['Score_x'] + merged_df['Score_y'] + merged_df['Score']\n",
    "    \n",
    "    # Replace 'Table_Number' with titles\n",
    "    tagsdf = replace_table_number_with_title(merged_df, titleslist)\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    tagsdf = tagsdf.drop(merged_df.columns.difference(['Ensemble_Score', 'Table_Name', 'Tags']), axis=1)\n",
    "\n",
    "    # Filter out contained words\n",
    "    tagsdf = eliminate_contained_words(tagsdf)\n",
    "    \n",
    "    # Filter the top tags for each table according to the ensemble score\n",
    "    tagsdf = topscorefilter(tagsdf, numberoftags)\n",
    "\n",
    "    return tagsdf\n",
    "\n",
    "def ensemble_pipeline(bucket, file_key, functions, vectorfuncs, cluster_count, numberoftags):\n",
    "\n",
    "    '''\n",
    "    Run the final pipeline to import, process, cluster, and obtain tags for each table in the spreadsheet.\n",
    "    \n",
    "    Parameters:\n",
    "        bucket (str): The S3 bucket name.\n",
    "        file_key (str): The file key directing to the location of the file within the S3 bucket.\n",
    "        functions (list): The list of functions to be applied during the import process.\n",
    "        vectorfuncs (list): The list of functions to be applied during the vectorization process.\n",
    "        cluster_count (int): The number of clusters.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: The dataframe with tags for each table.\n",
    "    '''\n",
    "    data, titleslist = import_pipeline(bucket, file_key)\n",
    "    dflist = pipeline(data, functions)\n",
    "    abbdf, defsdf = reference_grabber()\n",
    "\n",
    "    vectorizer_results = pipeline(dflist, vectorfuncs)\n",
    "\n",
    "    # Extract the first values (vectorized_texts_list) and second values (feature_names_list)\n",
    "    vectorized_texts_list = [x[0] for x in vectorizer_results]\n",
    "    feature_names_list = [x[1] for x in vectorizer_results]\n",
    "    del vectorizer_results\n",
    "    \n",
    "    tfidftagsdf = tfidf_pipeline(vectorized_texts_list, feature_names_list, abbdf, defsdf)\n",
    "    clustertagsdf = cluster_pipeline(dflist, vectorized_texts_list, cluster_count)\n",
    "    costagsdf = cos_pipeline(vectorized_texts_list, feature_names_list, abbdf, defsdf)\n",
    "    \n",
    "    # Create the final dataframe\n",
    "    print('Ensemble Dataframe')\n",
    "    tagsdf = generate_tagsdf(clustertagsdf, costagsdf, tfidftagsdf, titleslist, numberoftags)\n",
    "    location = get_characters_before_slash(file_key)\n",
    "    tagsdf['Location'] = location\n",
    "    tagsdf['Bucket'] = bucket\n",
    "    display(tagsdf)\n",
    "    \n",
    "    return tagsdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a3ec4ee-18ef-4588-9d63-bc3336643df8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''Getting file keys and bucket name for S3 import from local json. These will serve as different example use cases.  In practice, these file keys will be extracted from the json created by an sns trigger. These were examples used during development.'''\n",
    "file_name = 'file_keys.json'\n",
    "\n",
    "with open(file_name, 'r') as f:\n",
    "    file_keys = json.load(f)\n",
    "    \n",
    "bucket_name = file_keys['bucket_name']\n",
    "file_key_radcom = file_keys['file_key_radcom']\n",
    "file_key_boost = file_keys['file_key_boost']\n",
    "file_key_eks = file_keys['file_key_eks']\n",
    "file_key_prometheus = file_keys['file_key_prometheus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7b3386e-a758-48d7-b914-5072055ca956",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Radcom\n",
      "Ensemble Dataframe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tags</th>\n",
       "      <th>Ensemble_Score</th>\n",
       "      <th>Table_Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cscf</td>\n",
       "      <td>5.451571</td>\n",
       "      <td>dish-5G.enterprise-core.probes.subscriber.cdr-...</td>\n",
       "      <td>radcom</td>\n",
       "      <td>dq-metadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>impu</td>\n",
       "      <td>5.374294</td>\n",
       "      <td>dish-5G.enterprise-core.probes.subscriber.cdr-...</td>\n",
       "      <td>radcom</td>\n",
       "      <td>dq-metadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>origin realm</td>\n",
       "      <td>2.320212</td>\n",
       "      <td>dish-5G.enterprise-core.probes.subscriber.cdr-...</td>\n",
       "      <td>radcom</td>\n",
       "      <td>dq-metadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>traffic based ne configuration</td>\n",
       "      <td>1.363397</td>\n",
       "      <td>dish-5G.enterprise-core.probes.subscriber.cdr-...</td>\n",
       "      <td>radcom</td>\n",
       "      <td>dq-metadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>plane traffic based ne</td>\n",
       "      <td>1.340876</td>\n",
       "      <td>dish-5G.enterprise-core.probes.subscriber.cdr-...</td>\n",
       "      <td>radcom</td>\n",
       "      <td>dq-metadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>user plane traffic based</td>\n",
       "      <td>1.322030</td>\n",
       "      <td>dish-5G.enterprise-core.probes.subscriber.cdr-...</td>\n",
       "      <td>radcom</td>\n",
       "      <td>dq-metadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tool tip</td>\n",
       "      <td>5.218656</td>\n",
       "      <td>dish-5G.enterprise-core.probes.subscriber.cdr-...</td>\n",
       "      <td>radcom</td>\n",
       "      <td>dq-metadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stream id</td>\n",
       "      <td>1.053991</td>\n",
       "      <td>dish-5G.enterprise-core.probes.subscriber.cdr-...</td>\n",
       "      <td>radcom</td>\n",
       "      <td>dq-metadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>service operation</td>\n",
       "      <td>0.729475</td>\n",
       "      <td>dish-5G.enterprise-core.probes.subscriber.cdr-...</td>\n",
       "      <td>radcom</td>\n",
       "      <td>dq-metadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>amf based ne configuration</td>\n",
       "      <td>0.885609</td>\n",
       "      <td>dish-5G.enterprise-core.probes.subscriber.cdr-...</td>\n",
       "      <td>radcom</td>\n",
       "      <td>dq-metadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nas 5gmm cause</td>\n",
       "      <td>0.287324</td>\n",
       "      <td>dish-5G.enterprise-core.probes.subscriber.cdr-...</td>\n",
       "      <td>radcom</td>\n",
       "      <td>dq-metadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>amf ue id</td>\n",
       "      <td>0.170378</td>\n",
       "      <td>dish-5G.enterprise-core.probes.subscriber.cdr-...</td>\n",
       "      <td>radcom</td>\n",
       "      <td>dq-metadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rtp synchronization source</td>\n",
       "      <td>1.363050</td>\n",
       "      <td>dish-5G.enterprise-core.probes.subscriber.cdr-...</td>\n",
       "      <td>radcom</td>\n",
       "      <td>dq-metadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>originating id</td>\n",
       "      <td>1.169490</td>\n",
       "      <td>dish-5G.enterprise-core.probes.subscriber.cdr-...</td>\n",
       "      <td>radcom</td>\n",
       "      <td>dq-metadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rtp initial</td>\n",
       "      <td>1.044704</td>\n",
       "      <td>dish-5G.enterprise-core.probes.subscriber.cdr-...</td>\n",
       "      <td>radcom</td>\n",
       "      <td>dq-metadata</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Tags  Ensemble_Score   \n",
       "0                             cscf        5.451571  \\\n",
       "1                             impu        5.374294   \n",
       "2                     origin realm        2.320212   \n",
       "3   traffic based ne configuration        1.363397   \n",
       "4           plane traffic based ne        1.340876   \n",
       "5         user plane traffic based        1.322030   \n",
       "6                         tool tip        5.218656   \n",
       "7                        stream id        1.053991   \n",
       "8                service operation        0.729475   \n",
       "9       amf based ne configuration        0.885609   \n",
       "10                  nas 5gmm cause        0.287324   \n",
       "11                       amf ue id        0.170378   \n",
       "12      rtp synchronization source        1.363050   \n",
       "13                  originating id        1.169490   \n",
       "14                     rtp initial        1.044704   \n",
       "\n",
       "                                           Table_Name Location       Bucket  \n",
       "0   dish-5G.enterprise-core.probes.subscriber.cdr-...   radcom  dq-metadata  \n",
       "1   dish-5G.enterprise-core.probes.subscriber.cdr-...   radcom  dq-metadata  \n",
       "2   dish-5G.enterprise-core.probes.subscriber.cdr-...   radcom  dq-metadata  \n",
       "3   dish-5G.enterprise-core.probes.subscriber.cdr-...   radcom  dq-metadata  \n",
       "4   dish-5G.enterprise-core.probes.subscriber.cdr-...   radcom  dq-metadata  \n",
       "5   dish-5G.enterprise-core.probes.subscriber.cdr-...   radcom  dq-metadata  \n",
       "6   dish-5G.enterprise-core.probes.subscriber.cdr-...   radcom  dq-metadata  \n",
       "7   dish-5G.enterprise-core.probes.subscriber.cdr-...   radcom  dq-metadata  \n",
       "8   dish-5G.enterprise-core.probes.subscriber.cdr-...   radcom  dq-metadata  \n",
       "9   dish-5G.enterprise-core.probes.subscriber.cdr-...   radcom  dq-metadata  \n",
       "10  dish-5G.enterprise-core.probes.subscriber.cdr-...   radcom  dq-metadata  \n",
       "11  dish-5G.enterprise-core.probes.subscriber.cdr-...   radcom  dq-metadata  \n",
       "12  dish-5G.enterprise-core.probes.subscriber.cdr-...   radcom  dq-metadata  \n",
       "13  dish-5G.enterprise-core.probes.subscriber.cdr-...   radcom  dq-metadata  \n",
       "14  dish-5G.enterprise-core.probes.subscriber.cdr-...   radcom  dq-metadata  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boost\n",
      "Ensemble Dataframe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tags</th>\n",
       "      <th>Ensemble_Score</th>\n",
       "      <th>Table_Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>activity transfer</td>\n",
       "      <td>1.574646</td>\n",
       "      <td>dish_retail_dl.att.att_data_cdr</td>\n",
       "      <td>BOOST</td>\n",
       "      <td>dq-metadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>session type data</td>\n",
       "      <td>1.200856</td>\n",
       "      <td>dish_retail_dl.att.att_data_cdr</td>\n",
       "      <td>BOOST</td>\n",
       "      <td>dq-metadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>indicates home space</td>\n",
       "      <td>1.004838</td>\n",
       "      <td>dish_retail_dl.att.att_data_cdr</td>\n",
       "      <td>BOOST</td>\n",
       "      <td>dq-metadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>field 00</td>\n",
       "      <td>4.314727</td>\n",
       "      <td>dish_retail_dl.att.att_voice_cdr</td>\n",
       "      <td>BOOST</td>\n",
       "      <td>dq-metadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>number billing systems provide</td>\n",
       "      <td>1.378006</td>\n",
       "      <td>dish_retail_dl.att.att_voice_cdr</td>\n",
       "      <td>BOOST</td>\n",
       "      <td>dq-metadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>phone number billing systems</td>\n",
       "      <td>1.336692</td>\n",
       "      <td>dish_retail_dl.att.att_voice_cdr</td>\n",
       "      <td>BOOST</td>\n",
       "      <td>dq-metadata</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Tags  Ensemble_Score   \n",
       "0               activity transfer        1.574646  \\\n",
       "1               session type data        1.200856   \n",
       "2            indicates home space        1.004838   \n",
       "3                        field 00        4.314727   \n",
       "4  number billing systems provide        1.378006   \n",
       "5    phone number billing systems        1.336692   \n",
       "\n",
       "                         Table_Name Location       Bucket  \n",
       "0   dish_retail_dl.att.att_data_cdr    BOOST  dq-metadata  \n",
       "1   dish_retail_dl.att.att_data_cdr    BOOST  dq-metadata  \n",
       "2   dish_retail_dl.att.att_data_cdr    BOOST  dq-metadata  \n",
       "3  dish_retail_dl.att.att_voice_cdr    BOOST  dq-metadata  \n",
       "4  dish_retail_dl.att.att_voice_cdr    BOOST  dq-metadata  \n",
       "5  dish_retail_dl.att.att_voice_cdr    BOOST  dq-metadata  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EKS\n",
      "Ensemble Dataframe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tags</th>\n",
       "      <th>Ensemble_Score</th>\n",
       "      <th>Table_Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>container_memory_utilization</td>\n",
       "      <td>0.222829</td>\n",
       "      <td>dish_wireless.dp.container_insights</td>\n",
       "      <td>EKS</td>\n",
       "      <td>dq-metadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>node_diskio_io_serviced_async</td>\n",
       "      <td>-1.052498</td>\n",
       "      <td>dish_wireless.dp.container_insights</td>\n",
       "      <td>EKS</td>\n",
       "      <td>dq-metadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>json container features</td>\n",
       "      <td>-1.174894</td>\n",
       "      <td>dish_wireless.dp.container_insights</td>\n",
       "      <td>EKS</td>\n",
       "      <td>dq-metadata</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Tags  Ensemble_Score   \n",
       "0   container_memory_utilization        0.222829  \\\n",
       "1  node_diskio_io_serviced_async       -1.052498   \n",
       "2        json container features       -1.174894   \n",
       "\n",
       "                            Table_Name Location       Bucket  \n",
       "0  dish_wireless.dp.container_insights      EKS  dq-metadata  \n",
       "1  dish_wireless.dp.container_insights      EKS  dq-metadata  \n",
       "2  dish_wireless.dp.container_insights      EKS  dq-metadata  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prometheus\n",
      "Ensemble Dataframe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tags</th>\n",
       "      <th>Ensemble_Score</th>\n",
       "      <th>Table_Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>annotations converted prometheus labels</td>\n",
       "      <td>0.813660</td>\n",
       "      <td>dish-5G.wireless.prometheus-metrics</td>\n",
       "      <td>Metrics</td>\n",
       "      <td>dq-metadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kubernetes annotations converted prometheus</td>\n",
       "      <td>0.802712</td>\n",
       "      <td>dish-5G.wireless.prometheus-metrics</td>\n",
       "      <td>Metrics</td>\n",
       "      <td>dq-metadata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kubernetes labels converted prometheus</td>\n",
       "      <td>0.798182</td>\n",
       "      <td>dish-5G.wireless.prometheus-metrics</td>\n",
       "      <td>Metrics</td>\n",
       "      <td>dq-metadata</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Tags  Ensemble_Score   \n",
       "0      annotations converted prometheus labels        0.813660  \\\n",
       "1  kubernetes annotations converted prometheus        0.802712   \n",
       "2       kubernetes labels converted prometheus        0.798182   \n",
       "\n",
       "                            Table_Name Location       Bucket  \n",
       "0  dish-5G.wireless.prometheus-metrics  Metrics  dq-metadata  \n",
       "1  dish-5G.wireless.prometheus-metrics  Metrics  dq-metadata  \n",
       "2  dish-5G.wireless.prometheus-metrics  Metrics  dq-metadata  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.9 s, sys: 1.21 s, total: 37.1 s\n",
      "Wall time: 30.3 s\n"
     ]
    }
   ],
   "source": [
    "'''Execute the example cases by running the final ensemble pipeline.  Ensemble pipeline takes in the parameters, and outputs a dataframe that, in practice, will be uploaded to an AuroraDB to store the information for future querying.'''\n",
    "%%time\n",
    "#Compile the list of functions for the different steps\n",
    "functions = [df_header_fixer, df_trunkater, desc_filler, cleaner] \n",
    "vectorfuncs= [vectorizer]\n",
    "\n",
    "print('Radcom')\n",
    "radcom_df = ensemble_pipeline(bucket_name, file_key_radcom, functions, vectorfuncs, 1, 3)\n",
    "print('Boost')\n",
    "boost_df = ensemble_pipeline(bucket_name, file_key_boost, functions, vectorfuncs, 1, 3)\n",
    "print('EKS')\n",
    "eks_df = ensemble_pipeline(bucket_name, file_key_eks, functions, vectorfuncs, 1, 3)\n",
    "print('Prometheus')\n",
    "prometheus_df = ensemble_pipeline(bucket_name, file_key_prometheus, functions, vectorfuncs, 1, 3)\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
